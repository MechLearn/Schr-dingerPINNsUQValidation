{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "448e4e7d",
   "metadata": {},
   "source": [
    "# Explicacion del codigo, del primero avance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "354a439e",
   "metadata": {},
   "source": [
    "## Descripcion\n",
    "\n",
    "En este apartado se colocara una revision del codigo, para verificar que se entiende, que aspectos se deben estudiar y como podria hacerlo, empezaremos por secciones"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89b8b5e2",
   "metadata": {},
   "source": [
    "### Primera PARTE, Importacion de Librerias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c87e1921",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-27 11:38:15.693681: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-12-27 11:38:16.072810: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-12-27 11:38:16.893193: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "/home/mech/tf_gpu/venv/lib/python3.12/site-packages/keras/src/export/tf2onnx_lib.py:8: FutureWarning: In the future `np.object` will be defined as the corresponding NumPy scalar.\n",
      "  if not hasattr(np, \"object\"):\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'gpu_setup'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 6\u001b[39m\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtensorflow\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtf\u001b[39;00m \u001b[38;5;66;03m# importe de libreria tensorflow de deep learning\u001b[39;00m\n\u001b[32m      5\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mmatplotlib\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpyplot\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mplt\u001b[39;00m \u001b[38;5;66;03m# Libreria que permite hacer graficos mas adecuados\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mgpu_setup\u001b[39;00m \u001b[38;5;66;03m# importa el modulo gpu_setup.py para la configuracion de la gpu\u001b[39;00m\n\u001b[32m      7\u001b[39m tf.keras.utils.set_random_seed(\u001b[32m0\u001b[39m) \u001b[38;5;66;03m# en esta parte se define una semilla, esta permite conservar los mismos valores siempre que se haga la ejecucion\u001b[39;00m\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'gpu_setup'"
     ]
    }
   ],
   "source": [
    "import os # la herramienta de matematica con muchas funciones de esta indole\n",
    "import math #Importa la libreria de gestion de archivos \"os\" \n",
    "import numpy as np # Libreria para calculos en dataframes\n",
    "import tensorflow as tf # importe de libreria tensorflow de deep learning\n",
    "import matplotlib.pyplot as plt # Libreria que permite hacer graficos mas adecuados\n",
    "from mlutils import setup_gpu\n",
    "setup_gpu()\n",
    "\n",
    "tf.keras.utils.set_random_seed(0) # en esta parte se define una semilla, esta permite conservar los mismos valores siempre que se haga la ejecucion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "928cd6db",
   "metadata": {},
   "source": [
    "### Segunda PARTE, Definicion de TRIAL con estructura fisica, Planteamiento Fisico\n",
    "\n",
    "En esta parte el interes es crear una clase que permita a la red usar una funcion a diferenciar mas acorde con el problema fisico de pozo de potencial, es decir que la ayude con la estructura de la prediccion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d4ef424",
   "metadata": {},
   "outputs": [],
   "source": [
    "def trig_nodal_factor(x,n): ## Esta funcion debe recibir los puntos espaciales y los modos \n",
    "\n",
    "    n_tf = tf.cast(n , x.dtype) ## aca usamos un casting para que la version de tensorflow no tenga problemas, convertimos el tipo de dato de n, al tipo de dato que tenga x, tambien  cabe aclarar que el casting de tensorflow, recibe dos cosas, la variable y el tipo de dato\n",
    "    pi_tf = tf.constant (math.pi, dtype = x.dytpe) # garantizamos que esta constante sea Tensorflow\n",
    "\n",
    "    # Ahora procederemos a determinar el factor trigonometrico \n",
    "\n",
    "    sin_fijo = tf.sin(pi_tf * x)\n",
    "    sin_n =  tf.sin(n_tf*pi_tf*x)\n",
    "\n",
    "    # Para evitar divisiones por cero, se hace un ajuste, en donde garanticemos que esto nunca pase\n",
    "\n",
    "    ratio = sin_n / (sin_fijo + 1e-12) # Ese parche garantiza que nunca se divida por cero\n",
    "\n",
    "    # tf.where, funciona de la siguiente forma tf.where(condición, valor_si_verdadero, valor_si_falso)\n",
    "\n",
    "\n",
    "\n",
    "    return tf.where(tf.abs(sin_fijo) < 1e-6, n_tf, ratio) # Esto genera una condicion, si el valor del seno del denominador es cercano a cero, use el valor de n,sino use el general, por el limite de x cuando tiende a cero\n",
    "\n",
    "\n",
    "# =================================== Construccion de la Red ==============================#\n",
    "# Algo importante , es que aqui no se define el entrenamiento simplemente se construye el modelo\n",
    "\n",
    "def make_net(n = 1, hidden = 64, use_sine = True): #la funcion de construccion de la red, recibe 3 parametros, el modo, la cantidad de neuronas y finalmente un booleano de si debe usar sin o no\n",
    "\n",
    "    x_in = tf.keras.Input(shape = (1,), dtype = tf.float32) # en esta linea se determina los parametros de entrada, en este caso la entrada de x, el comando de input, requiere la forma \"shape\" y el tipo de dato\n",
    "\n",
    "## Ahora vienen las condiciones ## \n",
    "\n",
    "## ===== Caso 1 , activacion del seno ===== ##\n",
    "\n",
    "    if use_sine:\n",
    "    # Esta es la primera capa oculta de la red, va a recibir neuronas, funcion de activacion que se aplicara todas las neuronas, como se inicializa los pesos, se hace a traves del kernel_inizialiter\n",
    "        z = tf.keras.layers.Dense(\n",
    "            hidden,\n",
    "            activation = tf.math.sin,\n",
    "            kernel_initializer = \"glorot_uniform\", # aqui se inicializan los pesos, en paticular glorot_uniform usa pesos de una distribucion uniforme\n",
    "            bias_initializer = \"zeros\"\n",
    "        )(x_in)\n",
    "        # ahora vamos con las segunda capa\n",
    "        z = tf.keras.layers.Dense(\n",
    "            hidden,\n",
    "            activation = tf.math.sin,\n",
    "            kernel_initializer = \"glorot_uniform\",\n",
    "            bias_initializer = \"zeros\"\n",
    "        )(z)\n",
    "\n",
    "    else : \n",
    "        z = tf.keras.layers.Dense(\n",
    "            hidden,\n",
    "            activation = \"tanh\",\n",
    "            kernel_initializer = \"glorot_uniform\",\n",
    "            bias_initializer = \"zeros\"\n",
    "        )(x_in)\n",
    "\n",
    "        z = tf.keras.layers.Dense(\n",
    "            hidden,\n",
    "            activation = \"tanh\",\n",
    "            kernel_initializer = \"glorot_uniform\",\n",
    "            bias_initializer = \"zeros\"\n",
    "\n",
    "        )(z)\n",
    "\n",
    "    ## ============ Definicion de la salida =================## esta capa es Nθ​\n",
    "\n",
    "    out = tf.keras.layers.Dense(\n",
    "        1, # aca es uno porque es una salida\n",
    "        activation = None, # en la salida no aplica una funcion de activacion\n",
    "        kernel_initializer=\"glorot_uniform\",\n",
    "        bias_initializer=\"zeros\"\n",
    "    )(z) # recibe a z, para la salida\n",
    "\n",
    "    #\n",
    "    F = tf.keras.layers.Lambda(lambda x: trig_nodal_factor(x,n))(x_in) # Lambda es una capa sin pesos que sirve para meter matemáticas hechas a mano dentro del modelo.\n",
    "\n",
    "    psi = tf.keras.layers.Lambda(\n",
    "        lambda inputs: inputs[0] * (1-inputs[0]) * inputs[1] * inputs[2]\n",
    "    ) (x_in, F, out)\n",
    "\n",
    "    return tf.keras.model(inputs = x_in, outputs = psi)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34b15220",
   "metadata": {},
   "source": [
    "### Luego de construida la red, se procede a revisar la diferenciacion, es decir la derivada, en particular la segunda derivada"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "903e9049",
   "metadata": {},
   "outputs": [],
   "source": [
    "def second_derivate(model,x):   # la funcion recibe el modelo construido y el valor x a diferenciar con la derivada\n",
    "\n",
    "    x = tf.convert_to_tensor(x); x = tf.reshape(x, (-1,1))\n",
    "\n",
    " # Esta línea cumple dos funciones importantes:\n",
    "# (1) Convierte lo que entre como x en un tensor de TensorFlow.\n",
    "# (2) Mediante reshape((-1,1)) garantiza que x tenga la forma\n",
    "#     (numero_de_muestras, numero_de_variables), que es la estructura\n",
    "#     esperada por las capas densas de la red.\n",
    "# El -1 indica que TensorFlow calcula automáticamente el número de muestras,\n",
    "# mientras que el 1 fija que solo hay una variable independiente (x).\n",
    "\n",
    "    with tf.GradientTape(persistent= True) as t2 :\n",
    "        t2.watch(x)\n",
    "        with tf.GradientTape() as t1:\n",
    "            t1.watch(x) # watch(x) le dice a GradientTape que x es la variable independiente respecto a la cual se van a calcular derivadas.\n",
    "            psi = model(x)\n",
    "        psi_x = t1.gradient(psi, x)\n",
    "    psi_xx = t2.gradient(psi_x, x)\n",
    "\n",
    "    return psi, psi_xx\n",
    "\n",
    "\n",
    "\n",
    " # Se abre una cinta de gradientes externa (persistent=True) que permite\n",
    "# calcular derivadas de orden superior. Esta cinta registrará todas las\n",
    "# operaciones necesarias para obtener la segunda derivada respecto a x.\n",
    "           \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44390c0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def second_derivate(x):\n",
    "\n",
    "    x = tf.convert_to_tensor(x, dtype=tf.float32)\n",
    "    x = tf.reshape(x, (-1,1))\n",
    "\n",
    "    with tf.GradientTape(persistent=True) as t2:\n",
    "        t2.watch(x)\n",
    "        with tf.GradientTape() as t1:\n",
    "            t1.watch(x)\n",
    "            psi = 3*x**3\n",
    "        psi_x = t1.gradient(psi, x)\n",
    "\n",
    "    psi_xx = t2.gradient(psi_x, x)\n",
    "    tf.print(\"psi =\", psi)\n",
    "    tf.print(\"psi_xx =\", psi_xx)\n",
    "\n",
    "    \n",
    "    return psi, psi_xx\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cf780feb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([[54.]], shape=(1, 1), dtype=float32)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(<tf.Tensor: shape=(1, 1), dtype=float32, numpy=array([[81.]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(1, 1), dtype=float32, numpy=array([[54.]], dtype=float32)>)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "second_derivate(3)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv (3.12.3)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
